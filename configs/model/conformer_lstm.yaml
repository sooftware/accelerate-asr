encoder_dim: 256
num_encoder_layers: 17
num_decoder_layers: 2
num_attention_heads: 8
feed_forward_expansion_factor: 4
conv_expansion_factor: 2
input_dropout_p: 0.1
feed_forward_dropout_p: 0.1
attention_dropout_p: 0.1
conv_dropout_p: 0.1
decoder_dropout_p: 0.1
conv_kernel_size: 31
half_step_residual: True
max_length: 128
teacher_forcing_ratio: 1.0
cross_entropy_weight: 0.7
ctc_weight: 0.3
joint_ctc_attention: True
rnn_type: lstm
optimizer: adam